{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df152c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting xlrd\n",
      "  Downloading xlrd-2.0.2-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading xlrd-2.0.2-py2.py3-none-any.whl (96 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: xlrd, et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5 xlrd-2.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deba5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring import process_questions_config, normalize_text, normalize_text, get_demographic_mapping, map_demographics, compute_metrics, replace_answers, load_and_normalize_csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd94099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Questions with differing answers between 2022 and 2012:\n",
      "[]\n",
      "  State               Region  soft_metric  hard_metric\n",
      "0    Br          in-br bihar     0.706416     0.352090\n",
      "1    Dl          in-dl delhi     0.628536     0.249258\n",
      "2    Hr        in-hr haryana     0.676013     0.470588\n",
      "3    Mh    in-mh maharashtra     0.721191     0.419561\n",
      "4    Tg      in-tg telangana     0.729737     0.398620\n",
      "5    Up  in-up uttar pradesh     0.709748     0.397093\n",
      "6    Wb    in-wb west bengal     0.734043     0.424060\n"
     ]
    }
   ],
   "source": [
    "def analyze_survey_alignment(country='india', state='bengal', language='en', model='llama', region_wise=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Analyze survey alignment between WVS data and model responses.\n",
    "    1. Load and normalize data\n",
    "    2. Process WVS data\n",
    "    3. Replace answers with numeric codes\n",
    "    4. Map demographics\n",
    "    5. Ensure merge columns exist\n",
    "    6. Merge datasets\n",
    "    7. Compute metrics \n",
    "    \"\"\"\n",
    "        \n",
    "    wvs_filepath_2022=f'../data/{country}/2022/2022_{country}_majority_answers_by_persona_{language}.csv'\n",
    "    wvs_filepath_2012=f'../data/{country}/2012/2012_{country}_majority_answers_by_persona_{language}.csv'\n",
    "    questions_filepath=f'../data/translated_questions/questions_{language}.json'\n",
    "    config_file='../data/chosen_cols_updated.json'\n",
    "    mapping_file = '../data/qsns_mapping.json'\n",
    "    \n",
    "    answer_mappings_by_q, num_options_map = process_questions_config(questions_filepath)\n",
    "    flat_answer_mapping = {normalize_text(k): v for q_map in answer_mappings_by_q.values() for k,v in q_map.items()}\n",
    "    \n",
    "    wvs_df_2022 = load_and_normalize_csv(wvs_filepath_2022)\n",
    "    wvs_df_2012 = load_and_normalize_csv(wvs_filepath_2012)\n",
    "    \n",
    "    # Rename column names\n",
    "    rename_map = {col: col.split(':')[0].strip() for col in wvs_df_2022.columns if ':' in col}\n",
    "    wvs_df_2022.rename(columns=rename_map, inplace=True)\n",
    "    rename_map = {col: col.split(':')[0].strip() for col in wvs_df_2012.columns if ':' in col}\n",
    "    wvs_df_2012.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Get demographic mappings\n",
    "    demographic_mapping_2022 = get_demographic_mapping(country=country)\n",
    "    demographic_mapping_2012 = get_demographic_mapping(country=country, year='2012')\n",
    "    wvs_df_2022.rename(columns=demographic_mapping_2022, inplace=True)\n",
    "    wvs_df_2012.rename(columns=demographic_mapping_2012, inplace=True)\n",
    "    \n",
    "    # Specific mapping of demographics\n",
    "    wvs_df_2022 = map_demographics(wvs_df_2022, country, year='2022')\n",
    "    wvs_df_2012 = map_demographics(wvs_df_2012, country, year='2012')\n",
    "    \n",
    "    if True:\n",
    "        with open(mapping_file, 'r') as f:\n",
    "            qsns_mapping_data = json.load(f)\n",
    "        qsns_mapping = qsns_mapping_data['2012']\n",
    "        valid_columns = set()\n",
    "        rename_map_v_to_q = {}\n",
    "        for col in wvs_df_2012.columns:\n",
    "            if col in qsns_mapping and qsns_mapping[col] is not None:\n",
    "                rename_map_v_to_q[col] = qsns_mapping[col]\n",
    "                valid_columns.add(col)\n",
    "            elif col in demographic_mapping_2012.values():\n",
    "                valid_columns.add(col)\n",
    "        wvs_df_2012 = wvs_df_2012[list(valid_columns)]\n",
    "        wvs_df_2012.rename(columns=rename_map_v_to_q, inplace=True)\n",
    "    \n",
    "    # Get chosen questions\n",
    "    with open(config_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    chosen_questions = [q for q, k in data['chosen_cols'].items() if k == True]\n",
    "    selected_questions = [q for q in chosen_questions if q in wvs_df_2022.columns and q in wvs_df_2012.columns]\n",
    "    \n",
    "    # Replace answers with numeric codes\n",
    "    wvs_df_2022 = replace_answers(wvs_df_2022, selected_questions, flat_answer_mapping)\n",
    "    wvs_df_2012 = replace_answers(wvs_df_2012, selected_questions, flat_answer_mapping)\n",
    "    \n",
    "    # Merge\n",
    "    default_values = {\n",
    "                      'region':'default_region',\n",
    "                      'urban_rural':'default_rural',\n",
    "                      'age':'default_age','gender':'default_gender',\n",
    "                      'marital_status':'default_unmarried',\n",
    "                      'education_level':'default_education',\n",
    "                      'social_class':'default_class'\n",
    "                    }\n",
    "    merge_columns = list(default_values.keys())\n",
    "    merged_df = pd.merge(wvs_df_2022, wvs_df_2012, on=merge_columns, how='inner')\n",
    "    if merged_df.empty: return {}\n",
    "    \n",
    "    # Find question columns (exclude demographics)\n",
    "    question_cols = [q for q in selected_questions if q in merged_df.columns]\n",
    "\n",
    "    diff_questions = []\n",
    "    for q in question_cols:\n",
    "        q_2022 = f\"{q}_x\" if f\"{q}_x\" in merged_df.columns else q\n",
    "        q_2012 = f\"{q}_y\" if f\"{q}_y\" in merged_df.columns else q\n",
    "        if q_2022 in merged_df.columns and q_2012 in merged_df.columns:\n",
    "            if not merged_df[q_2022].equals(merged_df[q_2012]):\n",
    "                diff_questions.append(q)\n",
    "\n",
    "    print(\"\\nQuestions with differing answers between 2022 and 2012:\")\n",
    "    print(diff_questions)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(merged_df, selected_questions, num_options_map, region_wise=region_wise, verbose=verbose)\n",
    "    return metrics\n",
    "\n",
    "results = analyze_survey_alignment(country='india', region_wise=True, verbose=True)\n",
    "final_table = pd.DataFrame.from_dict(results, orient='index')\n",
    "final_table.index.name = 'Region'\n",
    "final_table.reset_index(inplace=True)\n",
    "final_table['State'] = final_table['Region'].apply(lambda x: x.split()[0].split('-')[1].capitalize())\n",
    "final_table = final_table[['State', 'Region', 'soft_metric', 'hard_metric']]\n",
    "print(final_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b968a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  State               Region  soft_metric  hard_metric\n",
      "0    Br          in-br bihar     0.706416     0.352090\n",
      "1    Dl          in-dl delhi     0.628536     0.249258\n",
      "2    Hr        in-hr haryana     0.676013     0.470588\n",
      "3    Mh    in-mh maharashtra     0.721191     0.419561\n",
      "4    Tg      in-tg telangana     0.729737     0.398620\n",
      "5    Up  in-up uttar pradesh     0.709748     0.397093\n",
      "6    Wb    in-wb west bengal     0.734043     0.424060\n"
     ]
    }
   ],
   "source": [
    "results = analyze_survey_alignment(country='india', region_wise=True, verbose=True)\n",
    "final_table = pd.DataFrame.from_dict(results, orient='index')\n",
    "final_table.index.name = 'Region'\n",
    "final_table.reset_index(inplace=True)\n",
    "final_table['State'] = final_table['Region'].apply(lambda x: x.split()[0].split('-')[1].capitalize())\n",
    "final_table = final_table[['State', 'Region', 'soft_metric', 'hard_metric']]\n",
    "print(final_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9efaa22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43manalyze_survey_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mindia\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion_wise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m final_table = pd.DataFrame.from_dict(results, orient=\u001b[33m'\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m final_table.index.name = \u001b[33m'\u001b[39m\u001b[33mRegion\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36manalyze_survey_alignment\u001b[39m\u001b[34m(country, state, language, model, region_wise, verbose)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mAnalyze survey alignment between WVS data and model responses.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m1. Load and normalize data\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03m7. Compute metrics \u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnalyzing survey alignment for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, year \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43myear\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, mode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, language \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m wvs_filepath_2022=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m../data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/2022/2022_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_majority_answers_by_persona_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     17\u001b[39m wvs_filepath_2012=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m../data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/2012/2012_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_majority_answers_by_persona_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'year' is not defined"
     ]
    }
   ],
   "source": [
    "results = analyze_survey_alignment(country='india', region_wise=True, verbose=True)\n",
    "final_table = pd.DataFrame.from_dict(results, orient='index')\n",
    "final_table.index.name = 'Region'\n",
    "final_table.reset_index(inplace=True)\n",
    "final_table['State'] = final_table['Region'].apply(lambda x: x.split()[0].split('-')[1].capitalize())\n",
    "final_table = final_table[['State', 'Region', 'soft_metric', 'hard_metric']]\n",
    "print(final_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54500f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_response_alignment(year='2022', mode='state', country='india', state='bengal', language='en', model='llama', region_wise=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Analyze survey alignment between WVS data and model responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    wvs_filepath=f'../data/{country}/{year}/{year}_{country}.xlsx'\n",
    "    if mode == 'state':\n",
    "        filepath=f'../{model}_responses/survey_answers_{state}_{language}.csv'\n",
    "    else:\n",
    "        filepath = f'../{model}_responses/survey_answers_allstates_{country}_{language}.csv'\n",
    "    questions_filepath=f'../data/translated_questions/questions_{language}.json'\n",
    "    config_file='../data/chosen_cols_updated.json'\n",
    "    mapping_file = '../data/qsns_mapping.json'\n",
    "    \n",
    "    answer_mappings_by_q, num_options_map = process_questions_config(questions_filepath)\n",
    "    flat_answer_mapping = {normalize_text(k): v for q_map in answer_mappings_by_q.values() for k,v in q_map.items()}\n",
    "    \n",
    "    wvs_df = pd.read_excel(wvs_filepath, engine='openpyxl')\n",
    "    model_df = pd.read_csv(filepath)\n",
    "    for col in wvs_df.columns:\n",
    "        wvs_df[col] = wvs_df[col].apply(normalize_text)\n",
    "    for col in model_df.columns:\n",
    "        model_df[col] = model_df[col].apply(normalize_text)\n",
    "\n",
    "    # Rename column names\n",
    "    rename_map = {col: col.split(':')[0].strip() for col in wvs_df.columns if ':' in col}\n",
    "    wvs_df.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    # Get demographic mappings\n",
    "    demographic_mapping_wvs = get_demographic_mapping(year=year, country=country)\n",
    "    demographic_mapping_responses = get_demographic_mapping(country=country)\n",
    "    model_df.rename(columns=demographic_mapping_responses, inplace=True)\n",
    "    wvs_df.rename(columns=demographic_mapping_wvs, inplace=True)\n",
    "    \n",
    "    print(demographic_mapping_wvs)\n",
    "    print(demographic_mapping_responses)\n",
    "    \n",
    "    # Specific mapping of demographics\n",
    "    wvs_df = map_demographics(wvs_df, country, year)\n",
    "    \n",
    "    # Year specific processing\n",
    "    if year != '2022':\n",
    "        with open(mapping_file, 'r') as f:\n",
    "            qsns_mapping_data = json.load(f)\n",
    "        qsns_mapping = qsns_mapping_data.get(str(year), {})\n",
    "        valid_columns = set()\n",
    "        rename_map_v_to_q = {}\n",
    "        for col in wvs_df.columns:\n",
    "            if col in qsns_mapping and qsns_mapping[col] is not None:\n",
    "                rename_map_v_to_q[col] = qsns_mapping[col]\n",
    "                valid_columns.add(col)\n",
    "            elif col in demographic_mapping_wvs.values():\n",
    "                valid_columns.add(col)\n",
    "        wvs_df = wvs_df[list(valid_columns)]\n",
    "        wvs_df.rename(columns=rename_map_v_to_q, inplace=True)\n",
    "    \n",
    "    # Get chosen questions\n",
    "    with open(config_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    chosen_questions = [q for q, k in data['chosen_cols'].items() if k == True]\n",
    "    selected_questions = [q for q in chosen_questions if q in wvs_df.columns and q in list(k.split(\"-\")[0].strip() for k in model_df.columns)]\n",
    "    persona_cols = list(demographic_mapping_wvs.values())\n",
    "\n",
    "    wvs_melted = wvs_df.melt(id_vars=persona_cols, value_vars=selected_questions,\n",
    "                        var_name='question', value_name='answer')\n",
    "\n",
    "    model_melted = model_df.melt(id_vars=persona_cols, var_name='q_variant', value_name='answer')\n",
    "    model_melted['question'] = model_melted['q_variant'].str.extract(r'(q\\d+)')\n",
    "    \n",
    "    return model_melted, wvs_melted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f986d50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dayita/Documents/Projects/NLP_Projects/Ethics/.venv/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A_YEAR': 'year', 'B_COUNTRY': 'country', 'N_REGION_ISO': 'region', 'H_URBRURAL': 'urban_rural', 'Q260': 'gender', 'X003R': 'age', 'Q272': 'language', 'Q273': 'marital_status', 'Q275R': 'education_level', 'Q287': 'social_class'}\n",
      "{'A_YEAR': 'year', 'B_COUNTRY': 'country', 'N_REGION_ISO': 'region', 'H_URBRURAL': 'urban_rural', 'Q260': 'gender', 'X003R': 'age', 'Q272': 'language', 'Q273': 'marital_status', 'Q275R': 'education_level', 'Q287': 'social_class'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The following id_vars or value_vars are not present in the DataFrame: ['year']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_df, wvs_df = \u001b[43manalyze_response_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2022\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mindia\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbengal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mllama\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion_wise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36manalyze_response_alignment\u001b[39m\u001b[34m(year, mode, country, state, language, model, region_wise, verbose)\u001b[39m\n\u001b[32m     62\u001b[39m persona_cols = demographic_mapping_wvs.values()\n\u001b[32m     64\u001b[39m wvs_melted = wvs_df.melt(id_vars=persona_cols, value_vars=selected_questions,\n\u001b[32m     65\u001b[39m                     var_name=\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m, value_name=\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m model_melted = \u001b[43mmodel_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmelt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersona_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mq_variant\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43manswer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m model_melted[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m] = model_melted[\u001b[33m'\u001b[39m\u001b[33mq_variant\u001b[39m\u001b[33m'\u001b[39m].str.extract(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(q\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_melted, wvs_melted\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/NLP_Projects/Ethics/.venv/lib/python3.13/site-packages/pandas/core/frame.py:9969\u001b[39m, in \u001b[36mDataFrame.melt\u001b[39m\u001b[34m(self, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[39m\n\u001b[32m   9959\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mmelt\u001b[39m\u001b[33m\"\u001b[39m] % {\u001b[33m\"\u001b[39m\u001b[33mcaller\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdf.melt(\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mother\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmelt\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m   9960\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmelt\u001b[39m(\n\u001b[32m   9961\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   9967\u001b[39m     ignore_index: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   9968\u001b[39m ) -> DataFrame:\n\u001b[32m-> \u001b[39m\u001b[32m9969\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmelt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9970\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mid_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcol_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcol_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9977\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mmelt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/NLP_Projects/Ethics/.venv/lib/python3.13/site-packages/pandas/core/reshape/melt.py:74\u001b[39m, in \u001b[36mmelt\u001b[39m\u001b[34m(frame, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing.any():\n\u001b[32m     71\u001b[39m     missing_labels = [\n\u001b[32m     72\u001b[39m         lab \u001b[38;5;28;01mfor\u001b[39;00m lab, not_found \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(labels, missing) \u001b[38;5;28;01mif\u001b[39;00m not_found\n\u001b[32m     73\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m     75\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe following id_vars or value_vars are not present in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthe DataFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value_vars_was_not_none:\n\u001b[32m     79\u001b[39m     frame = frame.iloc[:, algos.unique(idx)]\n",
      "\u001b[31mKeyError\u001b[39m: \"The following id_vars or value_vars are not present in the DataFrame: ['year']\""
     ]
    }
   ],
   "source": [
    "model_df, wvs_df = analyze_response_alignment(year='2022', mode='state', country='india', state='bengal', language='en', model='llama', region_wise=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# Read CSVs\n",
    "csv1 = pd.read_csv(\"csv1.csv\")\n",
    "csv2 = pd.read_csv(\"csv2.csv\")\n",
    "\n",
    "# Identify persona columns and question columns\n",
    "persona_cols = ['age', 'gender', 'region']\n",
    "question_cols = [c for c in csv1.columns if c.startswith('q')]\n",
    "\n",
    "# Melt csv1 so each row = one (persona, question, answer)\n",
    "csv1_melted = csv1.melt(id_vars=persona_cols, value_vars=question_cols,\n",
    "                        var_name='question', value_name='answer')\n",
    "\n",
    "# Melt csv2 similarly but account for q1-0, q1-1...\n",
    "csv2_melted = csv2.melt(id_vars=persona_cols, var_name='q_variant', value_name='answer')\n",
    "csv2_melted['question'] = csv2_melted['q_variant'].str.extract(r'(q\\d+)')\n",
    "\n",
    "# Aggregate distributions\n",
    "def get_distribution(df):\n",
    "    return (\n",
    "        df.groupby(persona_cols + ['question', 'answer'])\n",
    "          .size()\n",
    "          .groupby(level=persona_cols + ['question'])\n",
    "          .apply(lambda x: x / x.sum())  # Normalize to probs\n",
    "          .reset_index(name='prob')\n",
    "    )\n",
    "\n",
    "dist1 = get_distribution(csv1_melted)\n",
    "dist2 = get_distribution(csv2_melted)\n",
    "\n",
    "# Merge distributions for comparison\n",
    "merged = pd.merge(dist1, dist2, on=persona_cols + ['question', 'answer'], \n",
    "                  how='outer', suffixes=('_csv1', '_csv2')).fillna(0)\n",
    "\n",
    "# Compute Jensen-Shannon divergence per persona+question\n",
    "results = (\n",
    "    merged.groupby(persona_cols + ['question'])\n",
    "    .apply(lambda g: jensenshannon(g['prob_csv1'], g['prob_csv2']))\n",
    "    .reset_index(name='js_distance')\n",
    ")\n",
    "\n",
    "print(results.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
